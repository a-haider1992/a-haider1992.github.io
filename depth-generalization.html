<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Overfitting in Deep Learning Models</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        p {
            line-height: 1.6;
        }
        .content {
            max-width: 800px;
            margin: auto;
        }
        header {
            background-color: black;
            color: white;
            text-align: center;
        }
        .container {
            color: black;
            padding: 20px;
            border-radius: 8px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Deep Learning Best Practices</h1>
    </header>
    <div class="container"></div>
        
        <h2>Depth of Deep Learning Models (mainly data compression models like AutoEncoders) vs Model Generalization</h2>
        <p>The depth of a deep learning model refers to the number of layers in the neural network. While deeper models have the potential to learn more complex patterns, they are also more prone to overfitting. It is crucial to find a balance between model depth and generalization to ensure the model performs well on new, unseen data.</p>
        <p>There exists a relationship between the successive layers (or blocks) and the model generalization to unseen data of same task and/or different task. Unformtunately, this relationship depends on number of factors such as DL architecture, complexity of training data, and so on. In most cases, the relationship is empirically derived
            through plottinng the relationship between loss (or accuracy) vs number of epochs. Moreover, both training and validation are used to find a sweet spot between model overfitting and underfitting. However, the sweet spot is a tunable hyperparameter most of the times and might require xtensive trial-and-errors.
        Unformtunately, we don't have a mathematical formula to find thats weet spot. However, we can try mathematical induction to derive such a formula, but the number of contributing variables could be enormously large (number of hidden layers etc.).</p>
        Despite these exisitng challenges, there are some standard techniques to find that sweet spot quickly and these techniques are identified through execessive trial-end-error. Hence, here are 
    the startegies mentioned below:</p>
        <ul>
            <li>Using cross-validation to determine the optimal number of layers</li>
            <li>Implementing regularization techniques to prevent overfitting</li>
            <li>Monitoring validation performance to avoid excessive depth</li>
        </ul>
        <h2>AutoEncoders (Compression ratio vs Number of trainable parameters)</h2>
        <p>AutoEncoders are self-supervised generative models which compresses data in input space and projects it to some unobservable latent space. Data could be any real-world observations like images, audio, text etc. I am more interested in images and more specifically medical images, hence, I will use medical images here as examples as supporting evidences.</p>       
        
        
    </div>
</body>
</html>